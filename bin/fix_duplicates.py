import os
import json
from dotenv import load_dotenv
from astrapy import DataAPIClient

load_dotenv()

ASTRA_TOKEN = os.getenv("ASTRA_DB_APPLICATION_TOKEN")
ASTRA_ENDPOINT = os.getenv("ASTRA_DB_API_ENDPOINT")
COLLECTION_NAME = "movies2026"

if not all([ASTRA_TOKEN, ASTRA_ENDPOINT]):
    print("Error: Missing environment variables.")
    exit(1)

client = DataAPIClient(ASTRA_TOKEN)
db = client.get_database(ASTRA_ENDPOINT)
collection = db.get_collection(COLLECTION_NAME)

def fix_duplicate(title):
    print(f"\nScanning for: '{title}'")
    
    # Find records matching the title
    # Note: This relies on title matching exactly.
    results = list(collection.find({"title": title}, limit=10))
    
    if len(results) == 0:
        print("No records found.")
        return

    if len(results) == 1:
        doc = results[0]
        has_cast = 'cast' in doc or 'cast_details' in doc
        print(f"Only one record found: ID={doc['_id']}, Has Cast={has_cast}")
        if not has_cast:
            print(" -> This record effectively has no data. You should re-run the updated update script to populate it.")
        return

    print(f"Found {len(results)} duplicates:")
    
    # Scoring system to identify the "good" record
    # We prefer records with cast details and standard IDs
    best_doc = None
    best_score = -1
    docs_to_delete = []

    for doc in results:
        score = 0
        _id = doc['_id']
        
        # Check for data richness
        has_cast = 'cast' in doc or 'cast_details' in doc
        if has_cast: score += 20
        if 'keywords' in doc: score += 5
        if 'watch_providers' in doc: score += 5
        
        # Penalize ID formats that look like the ones generated by the old script if they are duplicates
        # (Assuming original data matches TMDB ID directly, e.g. "12345", and old script added "movie_")
        if str(_id).startswith("movie_"):
            score -= 1 

        print(f" - [{_id}] Score: {score}, Cast: {has_cast}")
        
        # Logic to separate best vs others
        if best_doc is None:
            best_doc = doc
            best_score = score
        else:
            if score > best_score:
                # Previous best is actually worse, mark for deletion
                docs_to_delete.append(best_doc)
                best_doc = doc
                best_score = score
            else:
                # Current match is worse than best, mark for deletion
                docs_to_delete.append(doc)

    # Execute cleanup
    if best_doc:
        print(f"Keeping BEST record: {best_doc['_id']}")
        
        for bad_doc in docs_to_delete:
            print(f"Deleting DUPLICATE: {bad_doc['_id']}")
            collection.delete_one({"_id": bad_doc['_id']})
            print(" -> Deleted.")

if __name__ == "__main__":
    print("Duplicate Fixer Tool")
    print("-------------------")
    print("Enter a movie title to check for duplicates (or 'q' to quit).")
    
    while True:
        title_input = input("\nEnter title: ").strip()
        if title_input.lower() in ['q', 'quit', 'exit']:
            break
            
        if not title_input:
            continue
            
        fix_duplicate(title_input)
